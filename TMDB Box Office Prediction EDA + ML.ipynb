{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TMDB Box Office Prediction EDA + ML\n\n![](https://cdn-images-1.medium.com/max/1200/1*vIR7iO-1GnY2xYxL6NiYkw.png)\n[image-source](https://cdn-images-1.medium.com/max/1200/1*vIR7iO-1GnY2xYxL6NiYkw.png)\n\nIn a world... where movies made an estimated $41.7 billion in 2018, the film industry is more popular than ever. But what movies make the most money at the box office? How much does a director matter? Or the budget? For some movies, it's \"You had me at 'Hello.'\" For others, the trailer falls short of expectations and you think \"What we have here is a failure to communicate.\"\n\nIn this competition, you're presented with metadata on over 7,000 past films from The Movie Database to try and predict their overall worldwide box office revenue. Data points provided include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries. You can collect other publicly available data to use in your model predictions, but in the spirit of this competition, use only data that would have been available before a movie's release.\n\n## *Kernel in progress, is continuously being updated and extended*"},{"metadata":{},"cell_type":"markdown","source":"## Preparations - Prerequisities"},{"metadata":{},"cell_type":"markdown","source":"![](https://images-na.ssl-images-amazon.com/images/I/91HTK796%2BML._SX425_.jpg)\n[image-source](https://images-na.ssl-images-amazon.com/images/I/91HTK796%2BML._SX425_.jpg)"},{"metadata":{},"cell_type":"markdown","source":"### Loading Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport gc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nsub_df = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inspecting the train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a variaty of data, numerical, categorical and even lists of json formats."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for NA values in trainset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"![](https://prod-discovery.edx-cdn.org/media/course/image/2102f79d-9a44-41e9-9d92-884bec46dc65-ff40350cad17.small.jpg)\n[image-source](https://prod-discovery.edx-cdn.org/media/course/image/2102f79d-9a44-41e9-9d92-884bec46dc65-ff40350cad17.small.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So many columns and features to investigate, lets start by inspecting one by one each feature."},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Revenue\nOur target variable to be predicted"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(3, figsize=(12,7))\nsns.set(rc={'figure.figsize':(12,8)})\nsns.boxplot(x=train_df.revenue, ax = ax[0])\nsns.distplot(a=train_df.revenue, kde = False, ax = ax[1])\nsns.distplot(a=np.log1p(train_df.revenue), kde = False, ax = ax[2])\nf.tight_layout()\n\ntrain_df[\"log_revenue\"] = np.log1p(train_df.revenue)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Budget"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(3, figsize=(12,7))\nsns.set(rc={'figure.figsize':(12,8)})\nsns.boxplot(x=train_df.budget, ax = ax[0])\nsns.distplot(a=train_df.budget, kde = False, ax = ax[1])\nsns.distplot(a=np.log1p(train_df.budget), kde = False, ax = ax[2])\nf.tight_layout()\n\ntrain_df[\"log_budget\"] = np.log1p(train_df.budget)\ntest_df[\"log_budget\"] = np.log1p(test_df.budget)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"def genres_preprocessing(elem):\n    string = str(elem)\n    str1 = string.replace(']','').replace('[','').replace('{','').replace('}','').replace('\\'','').replace(' ','').replace(\"name\", \"\").replace(\"id\", \"\").replace(\":\", \"\")\n    ll = str1.split(\",\")[1::2]\n    return ll\n\ntrain_df[\"genres_processed\"] = train_df.genres.apply(lambda elem: genres_preprocessing(elem))\ntest_df[\"genres_processed\"] = test_df.genres.apply(lambda elem: genres_preprocessing(elem))\n\ngenres_dict = dict()\n\nfor genre in train_df[\"genres_processed\"]:\n    for elem in genre:\n        if elem not in genres_dict:\n            genres_dict[elem] = 1\n        else:\n            genres_dict[elem] += 1\n\n\nsns.set(rc={'figure.figsize':(12,8)})\ngenres_df = pd.DataFrame.from_dict(genres_dict, orient='index')\ngenres_df.columns = [\"number_of_movies\"]\ngenres_df = genres_df.sort_values(by=\"number_of_movies\", ascending=False)\ngenres_df.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(9,8)})\ntrain_df['num_genres'] = train_df['genres_processed'].apply(lambda x: len(x) if x != {} else 0)\ntest_df['num_genres'] = test_df['genres_processed'].apply(lambda x: len(x) if x != {} else 0)\ntrain_df['num_genres'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### isGenre, creating new feature\nisDrama, isComedy etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"genres_df.index.values\nfor g in genres_df.index.values:\n    train_df['isGenre_' + g] = train_df['genres_processed'].apply(lambda x: 1 if g in x else 0)\n    test_df['isGenre_' + g] = test_df['genres_processed'].apply(lambda x: 1 if g in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Original Language"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.original_language.value_counts()[:10].plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Production Companies"},{"metadata":{"trusted":true},"cell_type":"code","source":"def production_companies_preprocessing(elem):\n    string = str(elem)\n    str1 = string.replace(']','').replace('[','').replace('{','').replace('}','').replace(' ','').replace(\"name\", \"\").replace(\"id\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\")\n    ll = str1.split(\",\")[0::2]\n    return ll\n\ntrain_df[\"production_companies_processed\"] = train_df.production_companies.apply(lambda elem: production_companies_preprocessing(elem))\n\nproduction_companies_dict = dict()\n\nfor production_company in train_df[\"production_companies_processed\"]:\n    for elem in production_company:\n        if elem not in production_companies_dict:\n            production_companies_dict[elem] = 1\n        else:\n            production_companies_dict[elem] += 1\n\n\nsns.set(rc={'figure.figsize':(12,8)})\nproduction_companies_df = pd.DataFrame.from_dict(production_companies_dict, orient='index')\nproduction_companies_df.columns = [\"number_of_movies\"]\nproduction_companies_df = production_companies_df.sort_values(by=\"number_of_movies\", ascending=False)\nproduction_companies_df.head(20).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### production_countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"def production_countries_preprocessing(elem):\n    string = str(elem)\n    str1 = string.replace(']','').replace('[','').replace('{','').replace('}','').replace(' ','').replace(\"name\", \"\").replace(\"iso_3166_1\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\")\n    ll = str1.split(\",\")[0::2]\n    return ll\n\ntrain_df[\"production_countries_processed\"] = train_df.production_countries.fillna(\"NaN\").apply(lambda elem: production_countries_preprocessing(elem))\ntest_df[\"production_countries_processed\"] = test_df.production_countries.fillna(\"NaN\").apply(lambda elem: production_countries_preprocessing(elem))\n\n\nproduction_countries_dict = dict()\n\nfor production_country in train_df[\"production_countries_processed\"]:\n    for elem in production_country:\n        if elem not in production_countries_dict:\n            production_countries_dict[elem] = 1\n        else:\n            production_countries_dict[elem] += 1\n\n\n\nproduction_countries_df = pd.DataFrame.from_dict(production_countries_dict, orient='index')\nproduction_countries_df.columns = [\"number_of_movies\"]\nproduction_countries_df = production_countries_df.sort_values(by=\"number_of_movies\", ascending=False)\nproduction_countries_df.head(20).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### popularity"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(3, figsize=(12,7))\nsns.boxplot(x=train_df.popularity, ax = ax[0])\nax[0].set_title(\"Popularity Boxplot\")\nsns.distplot(a=train_df.popularity, kde = False, ax = ax[1])\nax[1].set_title(\"Popularity Histogram\")\nsns.distplot(a=np.log1p(train_df.popularity), kde = False, ax = ax[2])\nax[2].set_title(\"Log1p transformed Popularity Histogram\")\nf.tight_layout()\n\ntrain_df[\"log_popularity\"] = np.log1p(train_df.popularity)\ntest_df[\"log_popularity\"] = np.log1p(test_df.popularity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Runtime"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"runtime\"] = train_df[\"runtime\"].fillna(train_df[\"runtime\"].mode()[0])\ntest_df[\"runtime\"] = test_df[\"runtime\"].fillna(test_df[\"runtime\"].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(4, figsize=(12,7))\n\ntrain_df.runtime = train_df.runtime.fillna(train_df.runtime.mode())\n\nsns.boxplot(x=train_df.runtime, ax = ax[0])\nax[0].set_title(\"Runtime Boxplot\")\nsns.distplot(a=train_df.runtime, kde = False, ax = ax[1])\nax[1].set_title(\"Runtime Histogram\")\nsns.distplot(a=train_df.runtime/360, kde = False, ax = ax[2])\nax[2].set_title(\"Runtime in Hours Histogram\")\nsns.distplot(a=np.log1p(train_df.runtime), kde = False, ax = ax[3])\nax[3].set_title(\"Log1p transformed Runtime Histogram\")\nf.tight_layout()\n\ntrain_df[\"runtime_in_hours\"] = train_df.runtime/360\ntest_df[\"runtime_in_hours\"] = test_df.runtime/360\n\ntrain_df[\"log_runtime\"] = np.log1p(train_df.runtime)\ntest_df[\"log_runtime\"] = np.log1p(test_df.runtime)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Release Date preprocessing before EDA and ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n\n# fill possible NA values with the statistical mode\ntrain_df[\"release_date\"] = train_df[\"release_date\"].fillna(train_df[\"release_date\"].mode()[0])\ntest_df[\"release_date\"] = test_df[\"release_date\"].fillna(test_df[\"release_date\"].mode()[0])\n\n\ntrain_df['temp'] = train_df.release_date.apply(lambda x: datetime.strptime(x, '%m/%d/%y'))\n\ntrain_df[\"month\"] = train_df.temp.apply(lambda x: x.month)\ntrain_df[\"year\"] = train_df.temp.apply(lambda x: x.year)\ntrain_df[\"day_of_week\"] = train_df.temp.apply(lambda x: x.weekday()+1)\n\ntrain_df = train_df.drop(['temp'], axis=1)\n\n\ntest_df['temp'] = test_df.release_date.apply(lambda x: datetime.strptime(x, '%m/%d/%y'))\n\ntest_df[\"month\"] = test_df.temp.apply(lambda x: x.month)\ntest_df[\"year\"] = test_df.temp.apply(lambda x: x.year)\ntest_df[\"day_of_week\"] = test_df.temp.apply(lambda x: x.weekday()+1)\n\ntest_df = test_df.drop(['temp'], axis=1)\n\n\n\ntrain_df[\"day_of_week\"] = train_df[\"day_of_week\"].fillna(train_df[\"day_of_week\"].mode()[0])\ntest_df[\"day_of_week\"] = test_df[\"day_of_week\"].fillna(test_df[\"day_of_week\"].mode()[0])\n\ntrain_df[\"year\"] = train_df[\"year\"].fillna(train_df[\"year\"].mode()[0])\ntest_df[\"year\"] = test_df[\"year\"].fillna(test_df[\"year\"].mode()[0])\n\ntrain_df[\"month\"] = train_df[\"month\"].fillna(train_df[\"month\"].mode()[0])\ntest_df[\"month\"] = test_df[\"month\"].fillna(test_df[\"month\"].mode()[0])\n\ntrain_df[[\"release_date\", \"month\", \"year\", \"day_of_week\"]].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Month of Release, which month has most of the releases"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,8)})\ntrain_df.month.value_counts().plot.bar()\nplt.title('Number of films per month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Day of Release, which day of the week has most of the releases"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,8)})\ntrain_df.day_of_week.value_counts().plot.bar()\nplt.title('Number of films per day_of_week')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Year of Release, which year has most of the releases"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20,8)})\ntrain_df.year.value_counts().plot.bar()\nplt.title('Number of films per year')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA - Bivariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Bivariate Analysis for numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(13,14)})\n\n# Compute the correlation matrix\ncorr = train_df[[\"revenue\", \"budget\", \"popularity\", \"runtime\"]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, mask=mask, \n            annot=True, \n            #fmt=\".2f\", \n            cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bivariate Analysis for log-transformed numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(13,14)})\n\n# Compute the correlation matrix\ncorr = train_df[[\"log_revenue\", \"log_budget\", \"log_popularity\", \"log_runtime\"]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, mask=mask, \n            annot=True, \n            #fmt=\".2f\", \n            cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis and Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"#### has_collection and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['has_collection'] = [0 if pd.isnull(x) else 1 for x in train_df['belongs_to_collection']]\ntest_df['has_collection'] = [0 if pd.isnull(x) else 1 for x in test_df['belongs_to_collection']]\nprint(train_df['has_collection'].value_counts())\n\nsns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='has_collection', y='revenue', data=train_df)\nplt.title('Revenue for film with and without being in a collection')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### homepage and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['has_homepage'] = [0 if pd.isnull(x) else 1 for x in train_df['homepage']]\ntest_df['has_homepage'] = [0 if pd.isnull(x) else 1 for x in test_df['homepage']]\nprint(train_df['has_homepage'].value_counts())\n\nsns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='has_homepage', y='revenue', data=train_df)\nplt.title('Revenue for film with and without homepage')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of Genres per movie and revenues"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_genres\"] = train_df.genres_processed.apply(len)\nprint(train_df[\"num_of_genres\"].value_counts())\n\nsns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='num_of_genres', y='revenue', data=train_df)\nplt.title('Revenues for films with multiple genres')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### original_language and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='original_language', y='revenue', data=train_df)\nplt.title('Revenue for a movie and its and original_language')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### production country and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_of_production_countries\"] = train_df.production_countries_processed.apply(len)\ntest_df[\"num_of_production_countries\"] = test_df.production_countries_processed.apply(len)\n\nprint(train_df[\"num_of_production_countries\"].value_counts())\n\nsns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='num_of_production_countries', y='revenue', data=train_df)\nplt.title('number of production countries for a movie and revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Day of the week when the movie released and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='day_of_week', y='revenue', data=train_df)\nplt.title('day_of_week when the movie release and revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Month when the movie released and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12, 8)})\nsns.boxplot(x='month', y='revenue', data=train_df)\nplt.title('month when the movie release and revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Year when the movie released and revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20, 8)})\ng = sns.boxplot(x='year', y='revenue', data=train_df)\nplt.xticks(rotation=90)\nplt.title('Year when the movie release and revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"![](https://cmci.colorado.edu/classes/INFO-4604/fa17/wordcloud.png)\n[image-source](https://cmci.colorado.edu/classes/INFO-4604/fa17/wordcloud.png)"},{"metadata":{},"cell_type":"markdown","source":"#### Preparations before ML modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_for_training = [\"log_budget\", \"log_popularity\", \"runtime\", \"day_of_week\", \"year\", \"month\", \"num_genres\", \"num_of_production_countries\", \"has_collection\", \"has_homepage\", \"original_language\", 'isGenre_Action',\n       'isGenre_Romance', 'isGenre_Crime', 'isGenre_Adventure',\n       'isGenre_Horror', 'isGenre_ScienceFiction', 'isGenre_Family',\n       'isGenre_Fantasy', 'isGenre_Mystery', 'isGenre_Animation',\n       'isGenre_History', 'isGenre_War', 'isGenre_Music',\n       'isGenre_Documentary', 'isGenre_Western', 'isGenre_Foreign',\n       'isGenre_TVMovie']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One Hot Encoding for the feature original_language"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n#for col in [\"original_language\"]:\n#    le=LabelEncoder()\n#    le.fit(list(train_df[col].fillna('')) + list(test_df[col].fillna('')))\n#    train_df[col] = le.transform(train_df[col].fillna('').astype(str))\n#    test_df[col] = le.transform(test_df[col].fillna('').astype(str))\n\n\n#gc.collect();\n\ntrain_and_test = pd.concat([train_df, test_df], axis=0)\ntrain_and_test = pd.concat([train_and_test, pd.get_dummies(train_and_test.original_language, prefix=\"original_language\")], axis=1)\ncolumns_for_training = columns_for_training + list(pd.get_dummies(train_and_test.original_language, prefix=\"original_language\").columns.values)\ntrain_df = train_and_test[~pd.isnull(train_and_test.revenue)]\ntest_df = train_and_test[pd.isnull(train_and_test.revenue)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[columns_for_training].head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_for_training.remove(\"original_language\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['log_revenue']\nX = train_df[columns_for_training]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline XGBoost modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport scikitplot as skplt\nimport time\nimport random\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\n\npredictions_test_xgb = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = 5\noof_rmse = 0\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_xgb = xgb.XGBRegressor(n_estimators=10000, seed=42, nthread=-1)\n\n    clf_stra_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    predictions_valid = clf_stra_xgb.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    oof_rmse += rmse_valid\n\n    predictions_test_xgb += clf_stra_xgb.predict(test_df[xtrain.columns])/num_of_splits\n\n\npredictions_test_xgb = np.expm1(predictions_test_xgb)\nprint(predictions_test)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nf.tight_layout()\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nsns.distplot(predictions_test_xgb, ax=ax[1])\n\nxgb.plot_importance(clf_stra_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bayesian Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bayesian_tuning(xtrain, ytrain):\n    \n    from skopt import BayesSearchCV\n    import xgboost as xgb\n    \n    \n    # Classifier\n    bayes_cv_tuner = BayesSearchCV(\n        estimator = xgb.XGBRegressor(\n            nthread = -1,\n            objective = 'reg:linear',\n            verbosity=1,\n            random_state=42\n        ),\n        search_spaces = {\n            'learning_rate': (0.01, 1.0),\n            'min_child_weight': (0, 10),\n            'n_estimators': (50, 100),\n            'max_depth': (0, 12),\n            'gamma': (1e-2, 10),\n            'subsample': (0.01, 1.0),\n            'colsample_bytree': (0.01, 1.0),\n            'colsample_bylevel': (0.01, 1.0),\n            'scale_pos_weight': (0.01, 1.0),\n            'reg_lambda': (1e-1, 10),\n            'reg_alpha': (1e-2, 1.0),\n            'max_delta_step': (0, 10),\n            'scale_pos_weight': (1e-2, 1)\n        },\n        cv = KFold(\n            n_splits=num_of_splits,\n            shuffle=True,\n            random_state=42\n        ),\n        n_jobs = 1,\n        n_iter = 9,   \n        verbose = 0,\n        refit = True,\n        random_state = 42\n    )\n\n    def status_print(optim_result):\n        \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n\n        # Get all the models tested so far in DataFrame format\n        all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n\n        # Get current parameters and the best parameters    \n        best_params = pd.Series(bayes_cv_tuner.best_params_)\n        print('Model #{}\\nBest score: {}\\nBest params: {}\\n'.format(\n            len(all_models),\n            np.round(bayes_cv_tuner.best_score_, 4),\n            bayes_cv_tuner.best_params_\n        ))\n        \n    result = bayes_cv_tuner.fit(xtrain, ytrain, callback = status_print)\n    return result\n    \n# Fit the model\n#xtrain, ytrain = prepare_for_tuning(X, y, type_of_training=type_of_training)\nresult = bayesian_tuning(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Training after tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nimport time\nimport random\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\npredictions_test_xgb_tuned = np.zeros(len(test_df))\nnum_fold = 0\noof_rmse = 0\nnum_of_splits = 5\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n    \n    \n    clf_stra_tuned_xgb = xgb.XGBRegressor(colsample_bytree = result.best_params_[\"colsample_bytree\"],\n                                    gamma=result.best_params_[\"gamma\"],                 \n                                    learning_rate=result.best_params_[\"learning_rate\"],\n                                    max_depth=result.best_params_[\"max_depth\"],\n                                    min_child_weight=result.best_params_[\"min_child_weight\"],\n                                    n_estimators=10000,\n                                    reg_alpha=result.best_params_[\"reg_alpha\"],\n                                    reg_lambda=result.best_params_[\"reg_lambda\"],\n                                    subsample=result.best_params_[\"subsample\"],\n                                    seed=42,\n                                    nthread = -1)\n\n    clf_stra_tuned_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    predictions_valid = clf_stra_tuned_xgb.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    oof_rmse += rmse_valid\n\n    predictions_test_xgb_tuned += clf_stra_tuned_xgb.predict(test_df[xtrain.columns])/num_of_splits\n    \nprint()\npredictions_test_xgb_tuned = np.expm1(predictions_test_xgb_tuned)\nprint(predictions_test_tuned)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nf.tight_layout()\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nsns.distplot(predictions_test_xgb_tuned, ax=ax[1])\n\nxgb.plot_importance(clf_stra_tuned_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Trees Baseline Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport scikitplot as skplt\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\n#predictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_extra_trees_test = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = 5\noof_rmse = 0\n#feature_importance_df = pd.DataFrame()\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_extra_trees = ExtraTreesRegressor(n_estimators=100, random_state=42)\n\n    clf_extra_trees.fit(xtrain_stra, ytrain_stra)\n\n    predictions_valid = clf_extra_trees.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold xvalid rmse:\", rmse_valid)\n    oof_rmse += rmse_valid\n\n    predictions_extra_trees_test += clf_extra_trees.predict(test_df[xtrain.columns])/num_of_splits\n\n\npredictions_extra_trees_test = np.expm1(predictions_extra_trees_test)\nprint()\nprint(predictions_extra_trees_test)\nprint()\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nf.tight_layout()\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nsns.distplot(predictions_extra_trees_test, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bayesian_tuning_extra_trees(xtrain, ytrain):\n    \n    from skopt import BayesSearchCV\n    from sklearn.ensemble import ExtraTreesRegressor\n    \n    \n    # Classifier\n    bayes_cv_tuner = BayesSearchCV(\n        estimator = ExtraTreesRegressor(\n            random_state=42\n        ),\n        search_spaces = {\n            'n_estimators': (10, 500),\n            'max_depth': (1, 12),\n            'min_samples_split': (2, 10),\n            'min_samples_leaf': (1, 10)\n        },\n        cv = KFold(\n            n_splits=num_of_splits,\n            shuffle=True,\n            random_state=42\n        ),\n        n_jobs = 1,\n        n_iter = 8,   \n        verbose = 0,\n        refit = True,\n        random_state = 42\n    )\n\n    def status_print(optim_result):\n        \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n\n        # Get all the models tested so far in DataFrame format\n        all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n\n        # Get current parameters and the best parameters    \n        best_params = pd.Series(bayes_cv_tuner.best_params_)\n        print('Model #{}\\nBest score: {}\\nBest params: {}\\n'.format(\n            len(all_models),\n            np.round(bayes_cv_tuner.best_score_, 4),\n            bayes_cv_tuner.best_params_\n        ))\n        \n    result_extra_trees = bayes_cv_tuner.fit(xtrain, ytrain, callback = status_print)\n    return result_extra_trees\n    \n# Fit the model\n#xtrain, ytrain = prepare_for_tuning(X, y, type_of_training=type_of_training)\nresult_extra_trees = bayesian_tuning_extra_trees(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_extra_trees.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Trees Training after tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport scikitplot as skplt\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\n#predictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_extra_trees_tuned_test = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = 5\noof_rmse = 0\n#feature_importance_df = pd.DataFrame()\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_extra_trees_tuned = ExtraTreesRegressor(random_state=42, \n                                                max_depth = result_extra_trees.best_params_['max_depth'], \n                                                min_samples_leaf = result_extra_trees.best_params_['min_samples_leaf'], \n                                                min_samples_split = result_extra_trees.best_params_['min_samples_split'], \n                                                n_estimators = result_extra_trees.best_params_['n_estimators'])\n\n    clf_extra_trees_tuned.fit(xtrain_stra, ytrain_stra)\n\n    predictions_valid = clf_extra_trees_tuned.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    print(\"Fold xvalid rmse:\", rmse_valid)\n    oof_rmse += rmse_valid\n\n    predictions_extra_trees_tuned_test += clf_extra_trees_tuned.predict(test_df[xtrain.columns])/num_of_splits\n\n\npredictions_extra_trees_tuned_test = np.expm1(predictions_extra_trees_tuned_test)\nprint()\nprint(predictions_extra_trees_tuned_test)\nprint()\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nf.tight_layout()\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nsns.distplot(predictions_extra_trees_tuned_test, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection with Eli5 for xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(clf_stra_xgb, random_state=42).fit(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(perm, feature_names = xvalid.columns.tolist(), top=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nmax_selected_features = 10\nsel = SelectFromModel(perm, max_features = max_selected_features, threshold=0.005, prefit=True)\n\nfeature_idx = sel.get_support()\nselected_feature_names = X.columns[feature_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_feature_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline XGBoost with Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nimport time\nimport random\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X[selected_feature_names], y, random_state=42, test_size=0.3)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\n#predictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_xgb_fs = np.zeros(len(test_df))\nnum_fold = 0\noof_rmse = 0\nnum_of_splits = 5\n#feature_importance_df = pd.DataFrame()\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n    \n    \n    clf_stra_fs_xgb = xgb.XGBRegressor(n_estimators=10000, seed=42, nthread = -1)\n\n    clf_stra_fs_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    predictions_valid = clf_stra_fs_xgb.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    oof_rmse += rmse_valid\n\n    predictions_test_xgb_fs += clf_stra_fs_xgb.predict(test_df[xtrain.columns])/num_of_splits\n    \n\npredictions_test_xgb_fs = np.expm1(predictions_test_xgb_fs)\nprint(predictions_test_xgb_fs)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nf.tight_layout()\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nsns.distplot(predictions_test_xgb_fs, ax=ax[1])\n\nxgb.plot_importance(clf_stra_fs_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tuning with feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = bayesian_tuning(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost training with Feature Selection and tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nimport time\nimport random\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X[selected_feature_names], y, random_state=42, test_size=0.3)\n\n#predictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_xgb_fs_tuned = np.zeros(len(test_df))\nnum_fold = 0\noof_rmse = 0\nnum_of_splits = 5\n#feature_importance_df = pd.DataFrame()\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n    \n    \n    clf_stra_fs_tuned_xgb = xgb.XGBRegressor(colsample_bytree = result.best_params_[\"colsample_bytree\"],\n                                    gamma=result.best_params_[\"gamma\"],                 \n                                    learning_rate=result.best_params_[\"learning_rate\"],\n                                    max_depth=result.best_params_[\"max_depth\"],\n                                    min_child_weight=result.best_params_[\"min_child_weight\"],\n                                    n_estimators=10000,\n                                    reg_alpha=result.best_params_[\"reg_alpha\"],\n                                    reg_lambda=result.best_params_[\"reg_lambda\"],\n                                    subsample=result.best_params_[\"subsample\"],\n                                    seed=42,\n                                    nthread = -1)\n\n    clf_stra_fs_tuned_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    predictions_valid = clf_stra_fs_tuned_xgb.predict(xvalid)\n    rmse_valid = np.sqrt(mean_squared_error(yvalid, predictions_valid))\n    oof_rmse += rmse_valid\n\n    predictions_test_xgb_fs_tuned += clf_stra_fs_tuned_xgb.predict(test_df[xtrain.columns])/num_of_splits\n    \n\npredictions_test_xgb_fs_tuned = np.expm1(predictions_test_xgb_fs_tuned)\nprint(predictions_test_xgb_fs_tuned)\nprint(\"OOF Out-of-fold rmse:\", oof_rmse/num_of_splits)\n\nf, ax = plt.subplots(2, figsize=(12,7))\n\nf.tight_layout()\nsns.set(rc={'figure.figsize':(9,14)})\nsns.distplot(train_df.revenue, ax=ax[0])\nsns.distplot(predictions_test_xgb_fs_tuned, ax=ax[1])\n\nxgb.plot_importance(clf_stra_fs_tuned_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ML Blends"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_test_xgb_extra_trees = (0.5 * predictions_test_xgb) + (0.5 * predictions_extra_trees_test)\npredictions_test_tuned_xgb_extra_trees = (0.5 * predictions_test_xgb_tuned) + (0.5 * predictions_extra_trees_tuned_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb baseline\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb\nsubmission.to_csv('clf_xgb_baseline.csv', index=False)\n\n# xgb tuning\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_tuned\nsubmission.to_csv('clf_xgb_tuned.csv', index=False)\n\n# extra trees baseline\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_extra_trees_test\nsubmission.to_csv('clf_extra_trees_baseline.csv', index=False)\n\n# extra trees tuning\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_extra_trees_tuned_test\nsubmission.to_csv('clf_extra_trees_tuned.csv', index=False)\n\n# xgb baseline with feature selection\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_fs\nsubmission.to_csv('clf_xgb_fs_baseline.csv', index=False)\n\n# xgb tuning with feature selection\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_fs_tuned\nsubmission.to_csv('clf_xgb_fs_tuned.csv', index=False)\n\n# Blend 1\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_xgb_extra_trees\nsubmission.to_csv('blend_xgb_extra_trees_baselines.csv', index=False)\n\n# Blend 2\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_tuned_xgb_extra_trees\nsubmission.to_csv('blend_xgb_extra_trees_tuned.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}