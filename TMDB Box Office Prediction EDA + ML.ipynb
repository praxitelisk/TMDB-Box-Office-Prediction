{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TMDB Box Office Prediction EDA + ML\n\n![](https://cdn-images-1.medium.com/max/1200/1*vIR7iO-1GnY2xYxL6NiYkw.png)\n[image-source](https://cdn-images-1.medium.com/max/1200/1*vIR7iO-1GnY2xYxL6NiYkw.png)"},{"metadata":{},"cell_type":"markdown","source":"## Prerequisities"},{"metadata":{},"cell_type":"markdown","source":"### Loading Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport gc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nsub_df = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inspecting the train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for NA values in trainset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Revenue"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2, figsize=(12,7))\nsns.boxplot(x=train_df.revenue, ax = ax[0])\nsns.distplot(a=train_df.revenue, kde = False, ax = ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Budget"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2, figsize=(12,7))\nsns.boxplot(x=train_df.budget, ax = ax[0])\nsns.distplot(a=train_df.budget, kde = False, ax = ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Genres"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef genres_preprocessing(elem):\n    string = str(elem)\n    str1 = string.replace(']','').replace('[','').replace('{','').replace('}','').replace('\\'','').replace(' ','').replace(\"name\", \"\").replace(\"id\", \"\").replace(\":\", \"\")\n    ll = str1.split(\",\")[1::2]\n    return ll\n\ntrain_df[\"genres_processed\"] = train_df.genres.apply(lambda elem: genres_preprocessing(elem))\n\ngenres_dict = dict()\n\nfor genre in train_df[\"genres_processed\"]:\n    for elem in genre:\n        if elem not in genres_dict:\n            genres_dict[elem] = 1\n        else:\n            genres_dict[elem] += 1\n\n\n\ngenres_df = pd.DataFrame.from_dict(genres_dict, orient='index')\ngenres_df.columns = [\"number_of_movies\"]\ngenres_df = genres_df.sort_values(by=\"number_of_movies\", ascending=False)\ngenres_df.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Original Language"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.original_language.value_counts()[:10].plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Production Companies"},{"metadata":{"trusted":true},"cell_type":"code","source":"def production_companies_preprocessing(elem):\n    string = str(elem)\n    str1 = string.replace(']','').replace('[','').replace('{','').replace('}','').replace(' ','').replace(\"name\", \"\").replace(\"id\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\")\n    ll = str1.split(\",\")[0::2]\n    return ll\n\ntrain_df[\"production_companies_processed\"] = train_df.production_companies.apply(lambda elem: production_companies_preprocessing(elem))\n\nproduction_companies_dict = dict()\n\nfor production_company in train_df[\"production_companies_processed\"]:\n    for elem in production_company:\n        if elem not in production_companies_dict:\n            production_companies_dict[elem] = 1\n        else:\n            production_companies_dict[elem] += 1\n\n\n\nproduction_companies_df = pd.DataFrame.from_dict(production_companies_dict, orient='index')\nproduction_companies_df.columns = [\"number_of_movies\"]\nproduction_companies_df = production_companies_df.sort_values(by=\"number_of_movies\", ascending=False)\nproduction_companies_df.head(20).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### production_countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"def production_countries_preprocessing(elem):\n    string = str(elem)\n    str1 = string.replace(']','').replace('[','').replace('{','').replace('}','').replace(' ','').replace(\"name\", \"\").replace(\"iso_3166_1\", \"\").replace(\":\", \"\").replace(\"\\'\", \"\")\n    ll = str1.split(\",\")[0::2]\n    return ll\n\ntrain_df[\"production_countries_processed\"] = train_df.production_countries.apply(lambda elem: production_countries_preprocessing(elem))\n\n\nproduction_countries_dict = dict()\n\nfor production_country in train_df[\"production_countries_processed\"]:\n    for elem in production_country:\n        if elem not in production_countries_dict:\n            production_countries_dict[elem] = 1\n        else:\n            production_countries_dict[elem] += 1\n\n\n\nproduction_countries_df = pd.DataFrame.from_dict(production_countries_dict, orient='index')\nproduction_countries_df.columns = [\"number_of_movies\"]\nproduction_countries_df = production_countries_df.sort_values(by=\"number_of_movies\", ascending=False)\nproduction_countries_df.head(20).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2, figsize=(12,7))\nsns.boxplot(x=train_df.popularity, ax = ax[0])\nsns.distplot(a=train_df.popularity, kde = False, ax = ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,8)})\n\n# Compute the correlation matrix\ncorr = train_df[[\"revenue\", \"budget\", \"popularity\", \"runtime\"]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, mask=mask, \n            annot=True, \n            #fmt=\".2f\", \n            cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['belongs_to_collection_flag'] = [0 if pd.isnull(x) else 1 for x in train_df['belongs_to_collection']]\ntrain_df['belongs_to_collection_flag'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['homepage_flag'] = [0 if pd.isnull(x) else 1 for x in train_df['homepage']]\ntrain_df['homepage_flag'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Preparations before ML modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['revenue']\nX = train_df[[\"budget\", \"popularity\", \"runtime\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A function to calculate Root Mean Squared Logarithmic Error (RMSLE)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(terms_to_sum) * (1.0/len(y))) ** 0.5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom collections import Counter\nfrom sklearn.metrics import mean_absolute_error\nimport scikitplot as skplt\nimport time\nimport random\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\n#predictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = 10\nmae = 0\n#feature_importance_df = pd.DataFrame()\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_xgb = xgb.XGBRegressor(colsample_bytree=0.4,\n                 gamma=0,                 \n                 learning_rate=0.07,\n                 max_depth=3,\n                 min_child_weight=1.5,\n                 n_estimators=10000,                                                                    \n                 reg_alpha=0.75,\n                 reg_lambda=0.45,\n                 subsample=0.6,\n                 seed=42)\n\n    clf_stra_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions_valid = clf_stra_xgb.predict(xvalid)\n    mae_valid = mean_absolute_error(yvalid, predictions_valid)\n    mae += mae_valid\n    #predictions_probas = clf_stra_xgb.predict_proba(xvalid)\n    #predictions_probas_list += predictions_probas/num_of_folds\n\n    predictions_test += clf_stra_xgb.predict(test_df[xtrain.columns])/10\n\n\n#predictions = np.argmax(predictions_probas, axis=1)\n\n\n\nprint(predictions_test)\nprint(mae/num_of_splits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bayesian Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bayesian_tuning(xtrain, ytrain):\n    \n    from skopt import BayesSearchCV\n    import xgboost as xgb\n    \n    \n    # Classifier\n    bayes_cv_tuner = BayesSearchCV(\n        estimator = xgb.XGBRegressor(\n            nthread = -1,\n            objective = 'reg:linear',\n            verbosity=1\n        ),\n        search_spaces = {\n            'learning_rate': (0.01, 1.0, 'log-uniform'),\n            'min_child_weight': (0, 10),\n            'n_estimators': (50, 100),\n            'max_depth': (0, 12),\n            'gamma': (1e-2, 10, 'log-uniform'),\n            'subsample': (0.01, 1.0, 'uniform'),\n            'colsample_bytree': (0.01, 1.0, 'uniform'),\n            'colsample_bylevel': (0.01, 1.0, 'uniform'),\n            'scale_pos_weight': (0.01, 1.0, 'uniform'),\n            'reg_lambda': (1e-1, 10, 'log-uniform'),\n            'reg_alpha': (1e-2, 1.0, 'log-uniform'),\n            'max_delta_step': (0, 10, 'uniform'),\n            'scale_pos_weight': (1e-2, 1, 'uniform')\n        },\n        cv = KFold(\n            n_splits=10,\n            shuffle=True,\n            random_state=42\n        ),\n        n_jobs = 1,\n        n_iter = 7,   \n        verbose = 0,\n        refit = True,\n        random_state = 42\n    )\n\n    def status_print(optim_result):\n        \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n\n        # Get all the models tested so far in DataFrame format\n        all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n\n        # Get current parameters and the best parameters    \n        best_params = pd.Series(bayes_cv_tuner.best_params_)\n        print('Model #{}\\nBest score: {}\\nBest params: {}\\n'.format(\n            len(all_models),\n            np.round(bayes_cv_tuner.best_score_, 4),\n            bayes_cv_tuner.best_params_\n        ))\n\n        ### Save all model results\n        #clf_name = bayes_cv_tuner.estimator.__class__.__name__\n        #all_models.to_csv(clf_name+\"_cv_results.csv\")\n        ###\n        \n    result = bayes_cv_tuner.fit(xtrain, ytrain, callback = status_print)\n    return result\n    \n# Fit the model\n#xtrain, ytrain = prepare_for_tuning(X, y, type_of_training=type_of_training)\nresult = bayesian_tuning(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training after tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nimport scikitplot as skplt\nimport time\nimport random\n\nimport xgboost as xgb\n\n\nprint(\"baseline\")\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\n#predictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_tuned = np.zeros(len(test_df))\nnum_fold = 0\nmae = 0\n#feature_importance_df = pd.DataFrame()\n\nfolds = KFold(n_splits=10, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n    \n    \n    clf_stra_tuned_xgb = xgb.XGBRegressor(colsample_bytree = result.best_params_[\"colsample_bytree\"],\n                                    gamma=result.best_params_[\"gamma\"],                 \n                                    learning_rate=result.best_params_[\"learning_rate\"],\n                                    max_depth=result.best_params_[\"max_depth\"],\n                                    min_child_weight=result.best_params_[\"min_child_weight\"],\n                                    n_estimators=10000,\n                                    reg_alpha=result.best_params_[\"reg_alpha\"],\n                                    reg_lambda=result.best_params_[\"reg_lambda\"],\n                                    subsample=result.best_params_[\"subsample\"],\n                                    seed=42)\n\n    clf_stra_tuned_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_tuned_xgb.predict(xvalid)\n    #predictions_probas = clf_stra_xgb.predict_proba(xvalid)\n    #predictions_probas_list += predictions_probas/num_of_folds\n\n    predictions_test_tuned += clf_stra_tuned_xgb.predict(test_df[xtrain.columns])/10\n\n\n#predictions = np.argmax(predictions_probas, axis=1)\n\n\n\nprint(predictions_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ntemp_train_df = train_df[[\"budget\", \"popularity\", \"runtime\"]]\ntemp_test_df = test_df[[\"budget\", \"popularity\", \"runtime\"]]\n\nscaler = StandardScaler()\ntemp_train_df = scaler.fit_transform(temp_train_df)\ntemp_test_df = scaler.transform(temp_test_df)\n\ntemp_train_df = pd.DataFrame(temp_train_df, columns=[\"budget\", \"popularity\", \"runtime\"])\ntemp_test_df = pd.DataFrame(temp_test_df, columns=[\"budget\", \"popularity\", \"runtime\"])\n\nX = temp_train_df\ny = train_df.revenue\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom collections import Counter\nfrom sklearn.metrics import mean_absolute_error\nimport scikitplot as skplt\nimport time\nimport random\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\n#predictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_scaled_test = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = 10\nmae = 0\n#feature_importance_df = pd.DataFrame()\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_scaled_xgb = xgb.XGBRegressor(colsample_bytree=0.4,\n                 gamma=0,                 \n                 learning_rate=0.07,\n                 max_depth=3,\n                 min_child_weight=1.5,\n                 n_estimators=10000,                                                                    \n                 reg_alpha=0.75,\n                 reg_lambda=0.45,\n                 subsample=0.6,\n                 seed=42)\n\n    clf_stra_scaled_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions_valid = clf_stra_scaled_xgb.predict(xvalid)\n    mae_valid = mean_absolute_error(yvalid, predictions_valid)\n    mae += mae_valid\n    #predictions_probas = clf_stra_xgb.predict_proba(xvalid)\n    #predictions_probas_list += predictions_probas/num_of_folds\n\n    predictions_scaled_test += clf_stra_scaled_xgb.predict(temp_test_df[xtrain.columns])/10\n\n\n#predictions = np.argmax(predictions_probas, axis=1)\n\n\n\nprint(predictions_scaled_test)\nprint(mae/num_of_splits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = bayesian_tuning(xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Retraining after scaling and tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom collections import Counter\nfrom sklearn.metrics import mean_absolute_error\nimport scikitplot as skplt\nimport time\nimport random\n\nimport xgboost as xgb\n\n# create a 70/30 stratified split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\n#predictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_scaled_tuned_test = np.zeros(len(test_df))\nnum_fold = 0\nnum_of_splits = 10\nmae = 0\n#feature_importance_df = pd.DataFrame()\n\nfolds = KFold(n_splits=num_of_splits, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_scaled_tuned_xgb = xgb.XGBRegressor(colsample_bytree = result.best_params_[\"colsample_bytree\"],\n                                    gamma=result.best_params_[\"gamma\"],                 \n                                    learning_rate=result.best_params_[\"learning_rate\"],\n                                    max_depth=result.best_params_[\"max_depth\"],\n                                    min_child_weight=result.best_params_[\"min_child_weight\"],\n                                    n_estimators=10000,\n                                    reg_alpha=result.best_params_[\"reg_alpha\"],\n                                    reg_lambda=result.best_params_[\"reg_lambda\"],\n                                    subsample=result.best_params_[\"subsample\"],\n                                    seed=42)\n\n    clf_stra_scaled_tuned_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=1000, eval_metric='rmse', verbose=100)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions_valid = clf_stra_scaled_tuned_xgb.predict(xvalid)\n    mae_valid = mean_absolute_error(yvalid, predictions_valid)\n    mae += mae_valid\n    #predictions_probas = clf_stra_xgb.predict_proba(xvalid)\n    #predictions_probas_list += predictions_probas/num_of_folds\n\n    predictions_scaled_tuned_test += clf_stra_scaled_tuned_xgb.predict(temp_test_df[xtrain.columns])/10\n\n\n#predictions = np.argmax(predictions_probas, axis=1)\n\n\nprint(predictions_scaled_tuned_test)\nprint(\"OOF mae: \",mae/num_of_splits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect();\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test\nsubmission.to_csv('clf_xgb_baseline.csv', index=False)\n\ngc.collect();\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_test_tuned\nsubmission.to_csv('clf_xgb_tuned.csv', index=False)\n\ngc.collect();\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_scaled_test\nsubmission.to_csv('clf_xgb_baseline_scaled_test.csv', index=False)\n\ngc.collect();\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['revenue'] = predictions_scaled_tuned_test\nsubmission.to_csv('clf_xgb_scaled_tuned.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}